{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e621d-172e-40cd-933c-8790b1940a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\"\"\"\n",
    "Missing values in a dataset refer to the absence of an observation or measurement in one or more variables. These values can occur due to various\n",
    "reasons such as data entry errors, faulty sensors, or incomplete data collection. \n",
    "\n",
    "It is essential to handle missing values in a dataset because they can significantly affect the analysis and modeling of the data. Missing values can \n",
    "distort the statistical measures such as mean, variance, and correlation and can also affect the accuracy of predictive models. \n",
    "\n",
    "Some algorithms that are not affected by missing values:\n",
    "\n",
    "1. Decision trees: Decision trees are not affected by missing values because they can work with incomplete data.\n",
    "2. Random Forest: Random Forest is an ensemble algorithm that is built on decision trees. Like decision trees, it is also not affected by missing \n",
    "          values.\n",
    "3. K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that can handle missing values by ignoring the missing data in distance calculation.\n",
    "4. Support Vector Machines (SVM): SVMs can handle missing values by simply ignoring the missing data during training.\n",
    "5. Naive Bayes: Naive Bayes algorithm can handle missing data by ignoring the missing values during calculation of the probability of the features.\n",
    "6. PCA (Principal Component Analysis): PCA can handle missing data by using the available data to calculate the principal components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d47fda-42c9-453f-86e1-20df83dd79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\"\"\"\n",
    "Here are some common techniques used to handle missing data:\n",
    "\n",
    "1. Removing rows with missing data: This technique involves deleting rows that have missing values. It is suitable when the number of missing values \n",
    "            is small compared to the total number of rows in the dataset \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "Filling missing values with mean/median/mode: This technique involves filling missing values with the mean/median/mode of the corresponding column. \n",
    "          It is suitable when the number of missing values is relatively small compared to the total number of rows in the dataset.\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, np.nan]})\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6b0a9-7080-4407-8f5f-e7d9af751c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\"\"\"\n",
    "Imbalanced data refers to a situation where the number of observations in one class is significantly different from the number of observations in the\n",
    "other class(es) in a classification problem. \n",
    "\n",
    "If imbalanced data is not handled properly, the model may become biased towards the majority class and perform poorly on the minority class. This is \n",
    "because the model will be optimized to maximize overall accuracy, which will lead to it prioritizing the majority class at the expense of the minority\n",
    "class. As a result, the model may predict the majority class most of the time, leading to low sensitivity or recall for the minority class. This could\n",
    "be particularly problematic if the minority class represents a critical outcome, such as detecting fraud or identifying a rare disease.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddaef6e-1c2d-4b5d-949f-8df4125bc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n",
    "\"\"\"\n",
    "Down-sampling involves randomly removing examples from the majority class to balance the dataset. This can be useful when the majority class has too \n",
    "     many examples and the dataset is too large to handle. \n",
    "Example: let's say we have a dataset with 1,000 examples of class A and 100 examples of class B. We could down-sample class A to have the same number \n",
    "     of examples as class B by randomly selecting 100 examples from class A.\n",
    "\n",
    "Up-sampling involves duplicating examples from the minority class to balance the dataset. This can be useful when the minority class has too few \n",
    "     examples and the model needs more data to learn from. \n",
    "Example, let's say we have a dataset with 1,000 examples of class A and 100 examples of class B. We could up-sample class B to have the same number of\n",
    "     examples as class A by duplicating each example in class B 10 times.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001636ec-3b0e-4af2-8666-9f551f7786b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: What is data Augmentation? Explain SMOTE.\n",
    "\"\"\"\n",
    "Data augmentation is a technique used in machine learning and deep learning to increase the size of a dataset by artificially creating new examples \n",
    "that are similar to the original data. The aim is to improve the robustness and generalization of a model by exposing it to a wider range of data that\n",
    "captures the variability of the real-world scenarios. Data augmentation is particularly useful when the available dataset is small or when it is not \n",
    "possible to collect more data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation algorithm that is used to address class imbalance problems in \n",
    "classification tasks. SMOTE works by creating synthetic examples for the minority class by interpolating between existing examples in that class. \n",
    "Specifically, for each example in the minority class, SMOTE selects one or more nearest neighbors from the same class and creates new examples along \n",
    "the line segments that connect the example and its neighbors in the feature space. The number of new examples to be created is determined by a \n",
    "user-defined parameter that specifies the desired degree of oversampling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21d8f8-f445-4e5d-8eae-71db4f03475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\"\"\"\n",
    "Outliers in a dataset are observations that are significantly different from other observations in the same dataset. These observations are either \n",
    "much larger or much smaller than the majority of the other observations in the dataset.\n",
    "\n",
    "It is important to identify and handle outliers because they can distort the results of statistical analyses and machine learning models. \n",
    "For example, outliers can lead to incorrect estimates of statistical measures such as mean and standard deviation, and they can also skew the \n",
    "distribution of the data. In machine learning, outliers can cause overfitting, where the model is trained to fit the outliers instead of the general \n",
    "pattern in the data, leading to poor performance on new data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283df91c-1d10-4f69-ada2-6436ca82da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some \n",
    "#    techniques you can use to handle the missing data in your analysis?\n",
    "\"\"\"\n",
    "There are several techniques that you can use to handle missing data in your analysis. Here are some common ones:\n",
    "1. Delete the missing data: One of the simplest methods is to delete the rows or columns with missing data. However, this approach may not be\n",
    "        appropriate if the missing data is substantial or systematic.\n",
    "2. Impute missing data: Another common approach is to impute the missing values with a reasonable estimate. You can use statistical methods like mean,\n",
    "        median, mode, or regression to fill in the missing data.\n",
    "3. Use a model-based imputation method: You can also use more sophisticated methods like expectation-maximization (EM) algorithm or multiple \n",
    "        imputations to estimate missing values based on the relationships among the variables.\n",
    "3. Use a hot-deck imputation method: In this method, you find a similar case to the one with missing data and use its value for imputing.\n",
    "4. Consider the reason for the missing data: Sometimes, the reason for missing data can be informative. In such cases, you can include the reason as a\n",
    "        variable in your analysis.\n",
    "5. Weighting: Assigning weights to your data points can give less weight to those with missing data, but still use the rest of the information to \n",
    "        derive insights.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d8e360-3710-47ec-8baa-0e09a8b16bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine \n",
    "#    if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "\"\"\"\n",
    "There are several strategies you can use to determine if missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "1. Visualization:One of the simplest ways to identify patterns in missing data is by visualizing it. You can create a plot that shows the distribution\n",
    "        of missing values across the dataset. If the missing data appears to be randomly distributed across the dataset, then it is likely missing at\n",
    "        random. However, if you notice any patterns or clusters of missing data, it may indicate that there is a systematic reason behind the missing \n",
    "        data.\n",
    "2. Correlation analysis: Another way to identify patterns in missing data is by looking at the correlation between the missing values and other \n",
    "        variables in the dataset. If there is a strong correlation between the missing data and other variables, it may indicate that there is a \n",
    "        pattern to the missing data.\n",
    "3. Imputation techniques: Another strategy to determine if the missing data is missing at random is to use imputation techniques. There are several \n",
    "        imputation techniques available, such as mean imputation, median imputation, and regression imputation. If the imputed values closely match \n",
    "        the actual values, then it is likely that the missing data is missing at random. However, if the imputed values differ significantly from the \n",
    "        actual values, it may indicate that there is a pattern to the missing data.\n",
    "4. Statistical tests: You can also use statistical tests to determine if missing data is missing at random. For example, you can perform a t-test or \n",
    "        ANOVA analysis on the complete cases versus the cases with missing data. If the results of the test are not significant, it may indicate that\n",
    "        the missing data is missing at random. However, if the results of the test are significant, it may indicate that there is a pattern to the \n",
    "        missing data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7e2de5-538e-441e-b6f1-5c41dd8e4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of \n",
    "#    interest,while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this \n",
    "#    imbalanced dataset?\n",
    "\"\"\"\n",
    "Here are some strategies to evaluate the performance of your model in such a scenario:\n",
    "\n",
    "1. Confusion Matrix: One way to evaluate the performance of a model on an imbalanced dataset is to use a confusion matrix. A confusion matrix can help\n",
    "        you visualize the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "2. ROC and AUC: Another way to evaluate the performance of your model on an imbalanced dataset is to use a Receiver Operating Characteristic (ROC) \n",
    "        curve and Area Under the Curve (AUC) metric. The ROC curve plots the true positive rate against the false positive rate for different \n",
    "        probability thresholds. The AUC metric provides a single number that represents the overall performance of the model.\n",
    "\n",
    "3. Precision, Recall and F1-Score: Precision, recall and F1-score are other metrics that can be used to evaluate the performance of your model. \n",
    "        Precision measures the fraction of true positive predictions among all positive predictions. Recall measures the fraction of true positives \n",
    "        that the model correctly identifies as positive. The F1-score is the harmonic mean of precision and recall. These metrics are useful when the \n",
    "        cost of false negatives and false positives is not equal.\n",
    "\n",
    "4. Resampling techniques: A common strategy to deal with imbalanced datasets is to use resampling techniques. One such technique is oversampling the \n",
    "        minority class, where you create additional synthetic samples of the minority class. Another technique is undersampling the majority class, \n",
    "        where you reduce the number of samples in the majority class. However, you must be careful not to introduce bias into the model while using \n",
    "        resampling techniques.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388b6aa4-7270-49ca-bb8c-f6440866d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers \n",
    "#     reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\"\"\"\n",
    "To balance an unbalanced dataset with a majority class, you can use various techniques to down-sample the majority class:\n",
    "\n",
    "1. Random under-sampling: randomly select a subset of the majority class to match the size of the minority class.\n",
    "\n",
    "2. Cluster-based under-sampling: cluster the majority class into smaller groups and select samples from each group to match the size of the minority \n",
    "        class.\n",
    "3. Tomek links: remove pairs of instances from different classes that are close to each other. This method can help to remove noisy samples from the\n",
    "        dataset.\n",
    "4. NearMiss: select samples from the majority class that are closest to the minority class based on distance measures.\n",
    "\n",
    "5. Condensed nearest neighbor: use a subset of the majority class that can correctly classify all samples in the minority class.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a9c68-d8ac-488c-bfab-cc0734ec1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the \n",
    "#     occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\"\"\"\n",
    "When dealing with imbalanced datasets, common methods can you employ to balance the dataset and up-sample the minority class are:\n",
    "\n",
    "1. Upsampling the minority class: This involves creating more samples of the minority class to increase its representation in the dataset. Some \n",
    "        popular methods for doing this include:\n",
    "2. Random oversampling: randomly duplicating samples from the minority class to create more balanced class distribution.\n",
    "\n",
    "3. Synthetic Minority Over-sampling Technique (SMOTE): this is a popular algorithm that generates synthetic samples by interpolating between existing \n",
    "        samples in the minority class.\n",
    "        \n",
    "4. ADASYN (Adaptive Synthetic Sampling): This is a variation of SMOTE that generates more synthetic samples near the decision boundary between the\n",
    "        minority and majority classes to increase the diversity of the minority class.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
