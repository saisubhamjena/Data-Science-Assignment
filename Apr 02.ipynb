{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20b42a-7615-4b9c-bdd6-6fe0b4f69351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\"\"\"\n",
    "Grid search CV is a technique used in machine learning to find the best combination of hyperparameters for a given model. It works by creating a grid\n",
    "of all possible hyperparameter combinations and evaluating the performance of the model for each combination using cross-validation. The \n",
    "hyperparameter combination that yields the highest score is selected as the best combination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc7ec3-4223-47a8-8add-22723a5f6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?\n",
    "\"\"\"\n",
    "Grid search CV exhaustively searches the entire hyperparameter space by creating a grid of all possible hyperparameter combinations and evaluating \n",
    "each combination using cross-validation. Grid search CV is guaranteed to find the best hyperparameter combination within the search space, but it can \n",
    "be computationally expensive, especially when the number of hyperparameters and their values is large.\n",
    "\n",
    "Randomized search CV samples the hyperparameter space randomly according to a predefined distribution. Randomized search CV may randomly sample 20 \n",
    "combinations from the search space and evaluate them using cross-validation. Randomized search CV is faster than grid search CV and can be used to \n",
    "search large hyperparameter spaces efficiently. However, here is no guarantee that the best hyperparameter combination will be found within the\n",
    "searchspace.\n",
    "\n",
    "The choice between grid search CV and randomized search CV depends on the size of the hyperparameter space and the computational resources available.\n",
    "If the hyperparameter space is small and computationally feasible to search exhaustively, grid search CV may be preferred to ensure the best \n",
    "hyperparameter combination is found. If the hyperparameter space is large and computational resources are limited, randomized search CV may be a more \n",
    "practical option.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7aa87-c50e-4107-b843-184161c459c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\"\"\"\n",
    "Data leakage is a problem in machine learning where information from outside of the training data is used to create a model, leading to overly \n",
    "optimistic performance estimates and poor generalization performance on new data. \n",
    "Data leakage occurs when information that would not be available in practice is used in the model training or evaluation process. This can happen \n",
    "intentionally or unintentionally, and it can occur in different stages of the machine learning pipeline, such as during data preprocessing, \n",
    "feature engineering, or model evaluation.\n",
    "\n",
    "Data leakage can lead to incorrect conclusions and ineffective models. For example, suppose a dataset contains a variable that is highly correlated\n",
    "with the target variable, but this variable is not available at prediction time. If the model is trained using this variable, it will perform well \n",
    "on the training data but poorly on new data, leading to overfitting. Another example is when a model is trained on the entire dataset, including the \n",
    "test set, leading to overfitting and unrealistic performance estimates.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e49421-935b-497f-9176-8d9f8244b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\"\"\"\n",
    "Some common techniques to prevent data leakage include:\n",
    "\n",
    "1. Holdout method: Splitting the dataset into training and testing sets and ensuring that no information from the testing set is used during training.\n",
    "\n",
    "2. Cross-validation: Repeatedly splitting the dataset into training and testing sets and ensuring that no information from the testing set is used \n",
    "    during training.\n",
    "\n",
    "3. Feature selection: Choosing features based only on information available in the training data and not using information from the testing set or\n",
    "    future data.\n",
    "\n",
    "4. Time-series split: Splitting the data in a time-ordered manner to ensure that the model is not using information from the future to make \n",
    "    predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0901a-4d9c-41f2-ad85-8c6f076bc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\"\"\"\n",
    "A confusion matrix is a table that summarizes the performance of a classification model on a set of test data for which the true values are known. \n",
    "The matrix consists of four different outcomes: \n",
    "1. True positive (TP) is when the model correctly predicts a positive class.\n",
    "2. False positive (FP) is when the model incorrectly predicts a positive class.  \n",
    "3. Frue negative (TN) is when the model correctly predicts a negative class.\n",
    "4. False negative (FN) is when the model incorrectly predicts a negative class.\n",
    "\n",
    "Here is an example of a confusion matrix for a binary classification problem:\n",
    "\n",
    "                   Predicted Positive\t| Predicted Negative\n",
    "                \n",
    "Actual Positive\t|  True Positive (TP)\t| False Negative (FN)\n",
    "Actual Negative\t|  False Positive (FP)\t| True Negative (TN)\n",
    "\n",
    "The confusion matrix can be used to calculate various performance metrics for the classification model, including:\n",
    "\n",
    "1. Accuracy: the proportion of correctly classified samples. \n",
    "    Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "2. Precision: the proportion of correctly predicted positive samples out of all predicted positive samples. \n",
    "    Precision = TP / (TP+FP)\n",
    "3. Recall (known as sensitivity or true positive rate): the proportion of correctly predicted positive samples out of all actual positive samples.\n",
    "    Recal = TP / (TP+FN)\n",
    "4. F1 score: a weighted average of precision and recall.\n",
    "    F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7644ea-6e10-4623-aacd-cd38ade5322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\"\"\"\n",
    "Precision tells us the proportion of the samples that we classified as positive that are actually positive. \n",
    "It is correctly identifying positive cases without incorrectly classifying negative cases as positive.\n",
    "Precision = TP / (TP+FP)\n",
    "\n",
    "Recall tells us the proportion of actual positive cases that are correctly identified as positive by the model. \n",
    "It is correctly identifying a high proportion of positive cases in the data.\n",
    "Recal = TP / (TP+FN)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167b9a7b-31e8-47b3-9b61-3e8e15ad40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\"\"\"\n",
    "To interpret the confusion matrix, we can look at the specific values of TP, FP, TN, and FN to gain insight into which types of errors the model \n",
    "is making. For example:\n",
    "\n",
    "1. If the model has a high number of false positives, it means that it is incorrectly classifying negative samples as positive. \n",
    "\n",
    "2. If the model has a high number of false negatives, it means that it is incorrectly classifying positive samples as negative. \n",
    "\n",
    "3. If the model has a high number of true positives and true negatives, it suggests that the model is performing well overall. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430f9b1-26d8-421b-b684-8ac06c48cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n",
    "\"\"\"\n",
    "Some common metrics that can be derived from a confusion matrix are:\n",
    "\n",
    "1. Accuracy: the proportion of correctly classified samples. \n",
    "    Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "2. Precision: the proportion of correctly predicted positive samples out of all predicted positive samples. \n",
    "    Precision = TP / (TP+FP)\n",
    "3. Recall (known as sensitivity or true positive rate): the proportion of correctly predicted positive samples out of all actual positive samples.\n",
    "    Recal = TP / (TP+FN)\n",
    "4. F1 score: a weighted average of precision and recall.\n",
    "    F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81b1d7-6464-48e7-b96b-f7006cc39899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\"\"\"\n",
    "Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "\n",
    "Accuracy is directily propertional to True positives and True negatives. \n",
    "A high number of true positives and true negatives will result in a higher accuracy.\n",
    "\n",
    "Accuracy is inversly propertional to False positives and False negatives. \n",
    "A high number of false positives and false negatives will result in a lower accuracy. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42388d7-65a4-4704-9bb7-c79e83696938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n",
    "\"\"\"\n",
    "A confusion matrix can be a powerful tool to help identify potential biases or limitations in a machine learning model, particularly when combined \n",
    "with other evaluation metrics such as precision, recall, F1 score, and ROC curves. Here are some ways you can use a confusion matrix to identify \n",
    "potential biases or limitations in your model:\n",
    "\n",
    "1. Class imbalance: If one class has significantly more samples than the other, the model may have a tendency to predict the majority class more\n",
    "    frequently, resulting in a high number of false negatives or false positives for the minority class. This can be detected by looking at the number\n",
    "    of samples in each class and the corresponding values in the confusion matrix.\n",
    "\n",
    "2. Bias towards certain features: If the model is biased towards certain features, it may over- or under-predict certain classes. This can be detected\n",
    "    by examining the features used in the model and the corresponding values in the confusion matrix.\n",
    "\n",
    "3. Limitations in the training data: If the training data is not representative of the target population, the model may not generalize well to new \n",
    "    data, resulting in poor performance on the test set. This can be detected by examining the distribution of the training data and comparing it to \n",
    "    the distribution of the test data.\n",
    "\n",
    "4. Limitations in the model architecture: If the model architecture is not suitable for the problem at hand, the model may not be able to learn the \n",
    "    relevant patterns in the data, resulting in poor performance. This can be detected by experimenting with different model architectures and \n",
    "    evaluating their performance using a confusion matrix and other metrics.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
